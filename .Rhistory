completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# The best point estimate for the sum is the average of all the sums.
final_point_estimate <- mean(list_of_sums)
# The uncertainty is captured by the standard deviation of these sums.
uncertainty_std_dev <- sd(list_of_sums)
# We create a 95% confidence interval using the quantiles of our distribution.
confidence_interval <- quantile(list_of_sums, probs = c(0.025, 0.975))
# Print the final results
cat("--- Final Results ---\n")
cat("Point Estimate for Total Ozone:", round(final_point_estimate, 2), "\n")
cat("Uncertainty (Standard Deviation of Sums):", round(uncertainty_std_dev, 2), "\n")
cat("95% Confidence Interval for Total Ozone: [",
round(confidence_interval[1], 2), ", ",
round(confidence_interval[2], 2), "]\n")
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 50 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 100 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
prop_to_remove <- 0.5
n_ozone_present <- sum(!is.na(data$Ozone))
n_to_remove <- floor(prop_to_remove * n_ozone_present)
# Get the indices of the currently non-missing Ozone values
indices_present <- which(!is.na(data$Ozone))
# Randomly sample from those indices to select which ones to set to NA
indices_to_remove <- sample(indices_present, size = n_to_remove)
# Create a new data frame for our experiment
data_missing <- data
data_missing$Ozone[indices_to_remove] <- NA
cat("Total number of missing Ozone values after removal:", sum(is.na(data_missing$Ozone)), "\n")
cat("Percentage of Ozone data now missing:",
round(sum(is.na(data_missing$Ozone)) / nrow(data_missing) * 100, 2), "%\n")
# 'm=50' means we create 50 versions of the completed dataset.
# 'pmm' (Predictive Mean Matching) is a good general-purpose imputation method.
# We set printFlag = FALSE to keep the output clean.
imputed_data <- mice(data_missing, m = 100, method = 'pmm', printFlag = FALSE)
# The 'imputed_data' object holds all 50 imputed datasets.
# print(imputed_data)
# We will create a vector to store the sum from each of the 50 datasets.
list_of_sums <- vector("numeric", length = imputed_data$m)
for (i in 1:imputed_data$m) {
# Get the i-th complete dataset
completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# The best point estimate for the sum is the average of all the sums.
final_point_estimate <- mean(list_of_sums)
# The uncertainty is captured by the standard deviation of these sums.
uncertainty_std_dev <- sd(list_of_sums)
# We create a 95% confidence interval using the quantiles of our distribution.
confidence_interval <- quantile(list_of_sums, probs = c(0.025, 0.975))
# Print the final results
cat("--- Final Results ---\n")
cat("Point Estimate for Total Ozone:", round(final_point_estimate, 2), "\n")
cat("Uncertainty (Standard Deviation of Sums):", round(uncertainty_std_dev, 2), "\n")
cat("95% Confidence Interval for Total Ozone: [",
round(confidence_interval[1], 2), ", ",
round(confidence_interval[2], 2), "]\n")
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 100 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
# We introduceren willekeurig ontbrekende data in ALLE kolommen.
prop_to_remove <- 0.30 # We verwijderen 30% van de bestaande, niet-NA data per kolom
# Maak een nieuwe data frame voor ons experiment
data_missing <- data
# Loop door elke kolom van de dataset
for (col_name in names(data_missing)) {
# Vind de indices van de waarden die nu aanwezig zijn
indices_present <- which(!is.na(data_missing[[col_name]]))
n_present <- length(indices_present)
# Bepaal hoeveel waarden we gaan verwijderen
n_to_remove <- floor(prop_to_remove * n_present)
# Voer de verwijdering alleen uit als er iets te verwijderen valt
if (n_to_remove > 0) {
# Selecteer willekeurig de indices die we naar NA gaan zetten
indices_to_remove <- sample(indices_present, size = n_to_remove)
# Zet de geselecteerde waarden op NA
data_missing[[col_name]][indices_to_remove] <- NA
}
}
# Toon een overzicht van de ontbrekende data na de verwijdering
cat("Overzicht van ontbrekende waarden (NAs) per kolom na verwijdering:\n")
print(sapply(data_missing, function(x) sum(is.na(x))))
cat("\nTotaal percentage ontbrekende cellen in de dataset:",
round(sum(is.na(data_missing)) / (nrow(data_missing) * ncol(data_missing)) * 100, 2), "%\n")
# 'm=50' means we create 50 versions of the completed dataset.
# 'pmm' (Predictive Mean Matching) is a good general-purpose imputation method.
# We set printFlag = FALSE to keep the output clean.
imputed_data <- mice(data_missing, m = 100, method = 'pmm', printFlag = FALSE)
# The 'imputed_data' object holds all 50 imputed datasets.
# print(imputed_data)
# We will create a vector to store the sum from each of the 50 datasets.
list_of_sums <- vector("numeric", length = imputed_data$m)
for (i in 1:imputed_data$m) {
# Get the i-th complete dataset
completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# The best point estimate for the sum is the average of all the sums.
final_point_estimate <- mean(list_of_sums)
# The uncertainty is captured by the standard deviation of these sums.
uncertainty_std_dev <- sd(list_of_sums)
# We create a 95% confidence interval using the quantiles of our distribution.
confidence_interval <- quantile(list_of_sums, probs = c(0.025, 0.975))
# Print the final results
cat("--- Final Results ---\n")
cat("Point Estimate for Total Ozone:", round(final_point_estimate, 2), "\n")
cat("Uncertainty (Standard Deviation of Sums):", round(uncertainty_std_dev, 2), "\n")
cat("95% Confidence Interval for Total Ozone: [",
round(confidence_interval[1], 2), ", ",
round(confidence_interval[2], 2), "]\n")
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 100 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
# We introduceren willekeurig ontbrekende data in ALLE kolommen.
prop_to_remove <- 0.5 # We verwijderen 30% van de bestaande, niet-NA data per kolom
# Maak een nieuwe data frame voor ons experiment
data_missing <- data
# Loop door elke kolom van de dataset
for (col_name in names(data_missing)) {
# Vind de indices van de waarden die nu aanwezig zijn
indices_present <- which(!is.na(data_missing[[col_name]]))
n_present <- length(indices_present)
# Bepaal hoeveel waarden we gaan verwijderen
n_to_remove <- floor(prop_to_remove * n_present)
# Voer de verwijdering alleen uit als er iets te verwijderen valt
if (n_to_remove > 0) {
# Selecteer willekeurig de indices die we naar NA gaan zetten
indices_to_remove <- sample(indices_present, size = n_to_remove)
# Zet de geselecteerde waarden op NA
data_missing[[col_name]][indices_to_remove] <- NA
}
}
# Toon een overzicht van de ontbrekende data na de verwijdering
cat("Overzicht van ontbrekende waarden (NAs) per kolom na verwijdering:\n")
print(sapply(data_missing, function(x) sum(is.na(x))))
cat("\nTotaal percentage ontbrekende cellen in de dataset:",
round(sum(is.na(data_missing)) / (nrow(data_missing) * ncol(data_missing)) * 100, 2), "%\n")
# 'm=50' means we create 50 versions of the completed dataset.
# 'pmm' (Predictive Mean Matching) is a good general-purpose imputation method.
# We set printFlag = FALSE to keep the output clean.
imputed_data <- mice(data_missing, m = 100, method = 'pmm', printFlag = FALSE)
# The 'imputed_data' object holds all 50 imputed datasets.
# print(imputed_data)
# We will create a vector to store the sum from each of the 50 datasets.
list_of_sums <- vector("numeric", length = imputed_data$m)
for (i in 1:imputed_data$m) {
# Get the i-th complete dataset
completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# The best point estimate for the sum is the average of all the sums.
final_point_estimate <- mean(list_of_sums)
# The uncertainty is captured by the standard deviation of these sums.
uncertainty_std_dev <- sd(list_of_sums)
# We create a 95% confidence interval using the quantiles of our distribution.
confidence_interval <- quantile(list_of_sums, probs = c(0.025, 0.975))
# Print the final results
cat("--- Final Results ---\n")
cat("Point Estimate for Total Ozone:", round(final_point_estimate, 2), "\n")
cat("Uncertainty (Standard Deviation of Sums):", round(uncertainty_std_dev, 2), "\n")
cat("95% Confidence Interval for Total Ozone: [",
round(confidence_interval[1], 2), ", ",
round(confidence_interval[2], 2), "]\n")
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 100 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
# We introduceren willekeurig ontbrekende data in ALLE kolommen.
prop_to_remove <- 0.90 # We verwijderen 30% van de bestaande, niet-NA data per kolom
# Maak een nieuwe data frame voor ons experiment
data_missing <- data
# Loop door elke kolom van de dataset
for (col_name in names(data_missing)) {
# Vind de indices van de waarden die nu aanwezig zijn
indices_present <- which(!is.na(data_missing[[col_name]]))
n_present <- length(indices_present)
# Bepaal hoeveel waarden we gaan verwijderen
n_to_remove <- floor(prop_to_remove * n_present)
# Voer de verwijdering alleen uit als er iets te verwijderen valt
if (n_to_remove > 0) {
# Selecteer willekeurig de indices die we naar NA gaan zetten
indices_to_remove <- sample(indices_present, size = n_to_remove)
# Zet de geselecteerde waarden op NA
data_missing[[col_name]][indices_to_remove] <- NA
}
}
# Toon een overzicht van de ontbrekende data na de verwijdering
cat("Overzicht van ontbrekende waarden (NAs) per kolom na verwijdering:\n")
print(sapply(data_missing, function(x) sum(is.na(x))))
cat("\nTotaal percentage ontbrekende cellen in de dataset:",
round(sum(is.na(data_missing)) / (nrow(data_missing) * ncol(data_missing)) * 100, 2), "%\n")
# 'm=50' means we create 50 versions of the completed dataset.
# 'pmm' (Predictive Mean Matching) is a good general-purpose imputation method.
# We set printFlag = FALSE to keep the output clean.
imputed_data <- mice(data_missing, m = 100, method = 'pmm', printFlag = FALSE)
# The 'imputed_data' object holds all 50 imputed datasets.
# print(imputed_data)
# We will create a vector to store the sum from each of the 50 datasets.
list_of_sums <- vector("numeric", length = imputed_data$m)
for (i in 1:imputed_data$m) {
# Get the i-th complete dataset
completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# We will create a vector to store the sum from each of the 50 datasets.
list_of_sums <- vector("numeric", length = imputed_data$m)
for (i in 1:imputed_data$m) {
# Get the i-th complete dataset
completed_df <- complete(imputed_data, i)
# Calculate the sum of Ozone and store it
list_of_sums[i] <- sum(completed_df$Ozone)
}
# 'list_of_sums' now contains 50 different possible total Ozone values.
# This vector represents the probability distribution of our desired total.
cat("Here are the first 6 calculated sums:\n")
head(list_of_sums)
# The best point estimate for the sum is the average of all the sums.
final_point_estimate <- mean(list_of_sums)
# The uncertainty is captured by the standard deviation of these sums.
uncertainty_std_dev <- sd(list_of_sums)
# We create a 95% confidence interval using the quantiles of our distribution.
confidence_interval <- quantile(list_of_sums, probs = c(0.025, 0.975))
# Print the final results
cat("--- Final Results ---\n")
cat("Point Estimate for Total Ozone:", round(final_point_estimate, 2), "\n")
cat("Uncertainty (Standard Deviation of Sums):", round(uncertainty_std_dev, 2), "\n")
cat("95% Confidence Interval for Total Ozone: [",
round(confidence_interval[1], 2), ", ",
round(confidence_interval[2], 2), "]\n")
# Visualize the distribution of the sums.
hist(list_of_sums,
breaks = 15,
main = "Distribution of possible total ozone sums\nacross 100 imputations",
xlab = "Calculated sum of ozone",
col = "lightblue",
border = "white")
# Add lines for the mean and confidence interval
abline(v = final_point_estimate, col = "red", lwd = 3)
abline(v = confidence_interval, col = "blue", lty = 2, lwd = 2)
abline(v = true_sum_of_Ozone, col = "black", lty = 2, lwd = 2)
legend("topright",
legend = c("Mean estimate", "95% CI", "True sum"),
col = c("red", "blue", "black"),
lty = c(1, 2),
lwd = 2,
bty = "n") # no box around legend
# --- 1. Install and Load the 'ape' Package ---
# The 'ape' (Analyses of Phylogenetics and Evolution) package is essential.
if (!requireNamespace("ape", quietly = TRUE)) {
install.packages("ape")
}
library(ape)
# --- 2. Define File Paths ---
# Set the path to your original, problematic tree file.
input_tree_path <- "D:\GTDB\ar53_r220.tree"
# --- 2. Define File Paths ---
# Set the path to your original, problematic tree file.
input_tree_path <- "D:/GTDB/ar53_r220.tree"
# Set the path for the new, cleaned tree file that will be created.
output_tree_path <- "D:/GTDB/ar53_r220_cleaned.tree"
# --- 3. Read and Write the Tree ---
cat("Reading the tree from:", input_tree_path, "\\n")
# The read.tree() function parses the Newick file into an R 'phylo' object.
# It is generally robust to minor formatting inconsistencies.
phylo_tree <- read.tree(input_tree_path)
cat("Writing the cleaned tree to:", output_tree_path, "\\n")
# --- 1. Install and Load Required Packages ---
# 'ape' is for phylogenetic analysis, 'dplyr' is for data manipulation.
if (!requireNamespace("ape", quietly = TRUE)) install.packages("ape")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(ape)
library(dplyr)
# --- 2. Define File Paths ---
# Path to the large Newick tree file (use the cleaned version if necessary)
tree_file_path <- "D:\GTDB\ar53_r220.tree"
# Path to the CSV file containing the list of genomes
accessions_csv_path <- "D:/pipeline_output/assembly_data_report_TaxId.csv"
# Path for the final, pruned tree file
output_pruned_tree_path <- "path/to/your/pruned_tree.tree"
# --- 3. Read the Tree and the Accession List ---
cat("Reading the full phylogenetic tree from:", tree_file_path, "\n")
full_tree <- read.tree(tree_file_path)
# --- 2. Define File Paths ---
# Path to the large Newick tree file (use the cleaned version if necessary)
tree_file_path <- "D:\GTDB\ar53_r220.tree"
# --- 2. Define File Paths ---
# Path to the large Newick tree file (use the cleaned version if necessary)
tree_file_path <- "D:/GTDB/ar53_r220.tree"
# Path to the CSV file containing the list of genomes
accessions_csv_path <- "D:/pipeline_output/assembly_data_report_TaxId.csv"
# Path for the final, pruned tree file
output_pruned_tree_path <- "path/to/your/pruned_tree.tree"
# --- 3. Read the Tree and the Accession List ---
cat("Reading the full phylogenetic tree from:", tree_file_path, "\n")
full_tree <- read.tree(tree_file_path)
cat("Reading the list of accessions from:", accessions_csv_path, "\n")
accessions_df <- read.csv(accessions_csv_path)
# --- 4. Prepare the List of Tips to Keep ---
# Extract the tip labels from the full tree
all_tree_tips <- full_tree$tip.label
# From your CSV, get the unique, non-missing accession numbers
base_accessions <- accessions_df %>%
filter(!is.na(accession) & accession != "") %>%
pull(accession) %>%
unique()
# Create the list of potential tip labels by adding GTDB prefixes
# This is crucial for matching the labels in the tree file.
prefixed_accessions <- c(
paste0("GB_", base_accessions),
paste0("RS_", base_accessions)
)
# Identify which of our accessions are actually present in the tree
tips_to_keep <- intersect(all_tree_tips, prefixed_accessions)
if (length(tips_to_keep) == 0) {
stop("Error: None of the accessions from the CSV were found in the tree. Please check your files.")
}
accessions_df
# --- 1. Install and Load Required Packages ---
# 'ape' is for phylogenetic analysis, 'readr' for fast CSV reading,
# and 'dplyr' for data manipulation.
if (!requireNamespace("ape", quietly = TRUE)) install.packages("ape")
if (!requireNamespace("readr", quietly = TRUE)) install.packages("readr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
library(ape)
library(dplyr)
# --- 2. Define File Paths ---
# Path to the large Newick tree file (use the cleaned version if necessary)
tree_file_path <- "D:/GTDB/ar53_r220.tree"
# Path to the CSV file containing the list of genomes
accessions_csv_path <- "D:/pipeline_output/assembly_data_report_TaxId.csv"
library(ape)
library(readr)
library(dplyr)
# --- 2. Define File Paths ---
# Path to the large Newick tree file (use the cleaned version if necessary)
tree_file_path <- "D:/GTDB/ar53_r220.tree"
# Path where the intermediate, cleaned tree file will be saved
cleaned_tree_path <- "D:/GTDB/ar53_r220.cleaned.tree"
# Path to the CSV file containing the list of genomes to keep
accessions_csv_path <- "D:/pipeline_output/assembly_data_report_TaxId.csv"
# Path for the final, pruned tree file
output_pruned_tree_path <- "ar53_pruned.tree"
# Path for the final, pruned tree file
output_pruned_tree_path <- "D:/GTDB/ar53_pruned.tree"
# --- 3. Clean the Original Tree File ---
# This is a critical first step to prevent parsing errors with large GTDB trees.
cat("Step 1: Cleaning the tree file to ensure compatibility...\\n")
original_tree <- read.tree(original_tree_path)
original_tree <- read.tree(tree_file_path)
write.tree(original_tree, file = cleaned_tree_path)
cat("Cleaned tree saved to:", cleaned_tree_path, "\\n\\n")
# --- 4. Read the Cleaned Tree and the Accession List ---
cat("Step 2: Reading the cleaned tree and the accession list...\\n")
full_tree <- read.tree(cleaned_tree_path)
accessions_df <- read_csv(accessions_csv_path)
# Extract all tip labels from the full tree
all_tree_tips <- full_tree$tip.label
# From your CSV, get the unique, non-missing accession numbers from the 'accession' column.
base_accessions <- accessions_df %>%
filter(!is.na(accession) & accession != "") %>%
pull(accession) %>%
unique()
# Create the list of potential tip labels by adding GTDB prefixes.
prefixed_accessions_to_keep <- c(
paste0("GB_", base_accessions),
paste0("RS_", base_accessions)
)
# Identify which of our accessions are actually present in the tree.
tips_to_keep <- intersect(all_tree_tips, prefixed_accessions_to_keep)
if (length(tips_to_keep) == 0) {
stop("Error: None of the accessions from the CSV were found in the tree. Please check your files and column name.")
}
library(ape)
library(readr)
library(dplyr)
library(stringr) # For string manipulation
# --- 3. Clean the Original Tree File ---
cat("Step 1: Cleaning the tree file to ensure compatibility...\\n")
original_tree <- read.tree(original_tree_path)
write.tree(original_tree, file = cleaned_tree_path)
# --- 3. Clean the Original Tree File ---
cat("Step 1: Cleaning the tree file to ensure compatibility...\\n")
original_tree <- read.tree(tree_file_path)
write.tree(original_tree, file = cleaned_tree_path)
cat("   -> Cleaned tree saved to:", cleaned_tree_path, "\\n\\n")
# --- 4. Read the Cleaned Tree and the Accession List ---
cat("Step 2: Reading the cleaned tree and the accession list...\\n")
full_tree <- read.tree(cleaned_tree_path)
accessions_df <- read_csv(accessions_csv_path)
cat("   -> Tree loaded with", length(full_tree$tip.label), "tips.\\n")
cat("   -> Accession file loaded with", nrow(accessions_df), "rows.\\n\\n")
# --- 5. Prepare a Robust List of Tips to Keep ---
cat("Step 3: Preparing a robust list of tips to keep...\\n")
all_tree_tips <- full_tree$tip.label
# From the CSV, get the unique accession numbers, stripping the GCF_/GCA_ part.
base_accession_numbers <- accessions_df %>%
filter(!is.na(accession) & accession != "") %>%
# Remove the GCF_ or GCA_ prefix to get just the numbers
mutate(number_part = str_replace(accession, "GC[AF]_", "")) %>%
pull(number_part) %>%
unique()
cat("   -> Found", length(base_accession_numbers), "unique accession numbers from the CSV.\\n")
# Generate all 4 possible variants for each number (RS/GB prefix + GCF/GCA infix)
potential_labels_to_keep <- c(
paste0("RS_GCF_", base_accession_numbers),
paste0("RS_GCA_", base_accession_numbers),
paste0("GB_GCF_", base_accession_numbers),
paste0("GB_GCA_", base_accession_numbers)
)
# Find the intersection between the tree's tips and our comprehensive list of possibilities.
tips_to_keep <- intersect(all_tree_tips, potential_labels_to_keep)
if (length(tips_to_keep) == 0) {
stop("Error: Even with robust matching, none of the accessions were found in the tree. Please double-check that your tree and accession files correspond to the same set of genomes.")
}
